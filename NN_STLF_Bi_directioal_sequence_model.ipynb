{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sdnnRxTNdyW1",
        "sYYk6NnFXqYr",
        "P8tzPKTojzwk",
        "la963BUT_u1v",
        "VLxvl0nvho20"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnnRxTNdyW1"
      },
      "source": [
        "# Neural Networks project\n",
        "## Short-Term Load Forecasting using Bi-directional Sequential Models and Feature Engineering for Small Datasets\n",
        "\n",
        "\n",
        "**Name**: *\\<Ali Nosouhi Dehnavi\\>*\n",
        "\n",
        "**Matricola**: *\\<1950716\\>*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNs-mS3WgzeO",
        "outputId": "5e56ce41-ac05-43c3-dd25-bd3649be8dec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEr8qV6-nMuL"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "from torch import nn, optim, as_tensor\n",
        "from torch.utils.data import Dataset, DataLoader,Subset\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import *\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable\n",
        "from matplotlib.dates import DateFormatter\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "plt.style.use('ggplot')\n",
        "from torchmetrics import MeanAbsolutePercentageError \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective"
      ],
      "metadata": {
        "id": "vn-x5IwqiO1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this Project is to forecast the future load consumption given a data set of load consumption history.Due to the volatile nature of electricity power consumption and its dependence on many parameters namely: 'time of the day','holiday','weather' and etc. the prediction is not an easy task.A briliant idea is to extract some hand-crafted features and use them along with raw data which led to better accuracy compared to use of only raw datas. In this paper a new architecture consisting two parallel path of recurrent neural networks is propsed.\n",
        "\n",
        " <figure>\n",
        " <center>\n",
        " <img src='https://i.postimg.cc/8cSVDg1W/Architechure.png' width=\"1000\" \n",
        "      height=\"500\"/>\n",
        " <figcaption>Proposed architecture 2 parallel recurrent layers</figcaption></center>\n",
        " </figure>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xETNYDjDHjrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pre-processing "
      ],
      "metadata": {
        "id": "t7Rr8Yq7ibO8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYYk6NnFXqYr"
      },
      "source": [
        "###  \n",
        "\n",
        "\n",
        "1.   First we read one of the CSV file from PERCON dataset and convert it from  1 minute to 30 minutes intervals taking the average of 30 successive samples.\n",
        "2.   since in evaluation phase MAPE(Mean Average Percentage Error) is sensetive to close-to-zero data we add a small value 0.1 kw to avoid non realistic result.\n",
        "3. we add extra columns for basic  **I**=[0 47], **D**=[0 6] **H**={0,1} and derived features **Avg**  **Std**  **Avg_w**  **Std_W**  which will be fed to two branches of network in parallel \n",
        "4. For simplicity we take  **window=2** for the expriment(this choice resulted better accuracy in the paper)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yae26jOmrdb",
        "outputId": "eab02b06-aa10-404e-9943-fc8ba42a2111"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir ='/content/drive/My Drive/NN project/data/sampled'  # source directory\n",
        "dest_dir='/content/drive/My Drive/NN project/data/processed'  # destination directory after preprocessing \n",
        "window=2\n",
        "for file in os.listdir(src_dir):\n",
        "  data = pd.read_csv(os.path.join(src_dir, file))\n",
        "  data['Date_Time'] = pd.to_datetime(data['Date_Time'], infer_datetime_format = True) #from  READING_DATETIME column in dataset(PERCON) to date-time\n",
        "\n",
        "  data['Usage_kW']=data['Usage_kW']+0.1   # add a small biase \n",
        "  data_30min=pd.DataFrame(0, index=np.arange(0,len(data)/30, dtype=int), columns=data.columns) # create a date frame with the same columns as original data\n",
        "\n",
        "  j=0\n",
        "  for k in  range(0, len(data)):\n",
        "    ts = data.loc[k, 'Date_Time']\n",
        "     \n",
        "    if (ts.minute == 0 or ts.minute == 30):\n",
        "        data_30min.loc[j,'Usage_kW']=data.loc[k:k+29, 'Usage_kW'].mean()\n",
        "        data_30min.loc[j,'Date_Time']=data.loc[k,'Date_Time']\n",
        "        j=j+1\n",
        "  \n",
        "  #data=data_30min \n",
        "  # expand the data frame with new null features with column labels I D H Avg Std Avg_w Std_w\n",
        "  data_30min['I'], data_30min['D'], data_30min['H'],data_30min['Avg'],data_30min['Std'],data_30min['Avg_w'],data_30min['Std_w'] = [0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "  # build an array with row dimension=the number of days  and columns dim=48(reading intervals per day) \n",
        "  arr = np.zeros((int(np.ceil(len(data_30min)/48)), 48))   \n",
        "   \n",
        "\n",
        "   #####################     Basic features     ################################\n",
        "\n",
        "   # for each day in data frame it satrts from 0 to 47 in column 'I'\n",
        "   # in case of missing datas corresponding  element [i,I-th] is left 0\n",
        "   # for the first and last day that we have not all measurment from 00:00 to 24:00  the elements of\n",
        "   # data.loc[i, 'I'] will receive values based on their hours(ts. hour) \n",
        "  for i in  range(0, len(data_30min)): \n",
        "    ts = data_30min.loc[i, 'Date_Time']    \n",
        "   \n",
        "    if (ts.minute == 0):\n",
        "        I = (ts.hour * 2) + 0\n",
        "    elif (ts.minute == 30):\n",
        "        I = (ts.hour * 2) + 1\n",
        "        \n",
        "    data_30min.loc[i, 'I'] = I\n",
        "        \n",
        "    # weekdays in coulmun 'D' satrts from 0 to 6 in column \n",
        "    weekday = ts.weekday()\n",
        "    data_30min.loc[i, 'D'] = weekday\n",
        "    \n",
        "    # Holidays column 'H' is set to 1 for saturdays(D=5) and sundays(D=6)\n",
        "    if (weekday > 4):\n",
        "        data_30min.loc[i, 'H'] = 1\n",
        "        \n",
        "#####################     Derived features     ################################\n",
        "    #current day number  \n",
        "    day = int(i/48)\n",
        "    #compute the avg and std for the same time of the current and  previous days(limited to window length [K in paper])\n",
        "    if(day >= window):\n",
        "      data_30min.loc[i, 'Avg'] = np.mean(arr[day-window:day, I])\n",
        "      data_30min.loc[i, 'Std'] = np.std(arr[day-window:day, I])\n",
        "\n",
        "    arr[day, I] = data.loc[i, 'Usage_kW'] # fill bar array from data frame\n",
        "\n",
        "    #compute avg and std of window length of preceding  time steps (window length) in the same day\n",
        "    if(i >= window - 1):\n",
        "      window_values = data_30min.loc[i+1-window: i, 'Usage_kW'].values\n",
        "      data_30min.loc[i, 'Avg_w'] = np.mean(window_values)\n",
        "      data_30min.loc[i, 'Std_w'] = np.std(window_values)    \n",
        "  \n",
        "  \n",
        "  data_30min.rename(columns={'Usage_kW': 'E'}, inplace=True)\n",
        "  data_30min = data_30min[['Date_Time', 'E', 'I', 'D', 'H', 'Avg', 'Std', 'Avg_w', 'Std_w']]\n",
        "  data=data_30min\n",
        "  data.to_csv(os.path.join(dest_dir, file)) # write new data frame in destination directory\n",
        "      "
      ],
      "metadata": {
        "id": "bNeb9hmkb7n5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After pre-processing new data set contains basic and derived features which will be used in proposed architecture**:\n",
        "\n",
        "\n",
        " <figure>\n",
        " <center>\n",
        " <img src='https://i.postimg.cc/433L0z0p/data-preprocessing.png\\\\' width=\"2000\" \n",
        "      height=\"400\"/>\n",
        " <figcaption>data after preprocess with  basic and derived features</figcaption></center>\n",
        " </figure>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OYn_U_6kccpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data frame to tensor** in order to become usable for pytorch models"
      ],
      "metadata": {
        "id": "xkpieXk95rmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_notime = data[[ 'E', 'I', 'D', 'H', 'Avg', 'Std', 'Avg_w', 'Std_w']]\n",
        "data_ten=torch.tensor(data_notime.values,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "CrdA35u4lkjm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation \n"
      ],
      "metadata": {
        "id": "P8tzPKTojzwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given cleaned dataset from previous section we are now able to produce $x_{train}$ ,$x_{train-derived}$, $y_{train}$ , $x_{test}$ , $x_{train-derived}$ , $y_{test}$\n",
        "\n",
        "\n",
        "\n",
        "1.   first we normalize the reading columns and featured data\n",
        "2.   use one-hot-encoding to create sets of sequential input batch for train and test\n",
        "\n"
      ],
      "metadata": {
        "id": "ye8XDhpdj8XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split=[0.9,0.1] #0.9 0.1"
      ],
      "metadata": {
        "id": "sMh3oJjefvSI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(data,data_type, window,split):\n",
        "    \n",
        "    mRange = int(len(data)*split[0])\n",
        "    dtrain = data[:mRange]\n",
        "    dtest = data[mRange:]\n",
        "        \n",
        "    if(data_type=='train'):\n",
        "      data = dtrain\n",
        "\n",
        "    else:\n",
        "      data = dtest\n",
        "\n",
        "    #Basic features\n",
        "    E=data[:,0]\n",
        "    I=data[:,1]\n",
        "    D=data[:,2]\n",
        "    H= data[:,3]\n",
        "\n",
        "    E_minmax = [torch.min(E),torch.max(E)] \n",
        "\n",
        "    E=(E-torch.min(E))/(torch.max(E)-torch.min(E))\n",
        "    E = E.reshape(-1, 1)\n",
        "\n",
        "    #I=[0....47] and after OHE it will be an array of binary with number of samples as row number\n",
        "    #and 48 columns(48 bits)    \n",
        "    I=F.one_hot(I.long())\n",
        "    #D=[0....6] and after OHE it will be an array of binary with number of samples as row number\n",
        "    #and 7 columns(7 bits) \n",
        "    D=F.one_hot(D.long())\n",
        "    #H=[0,1] and after OHE it will be an array of binary with number of samples as row number\n",
        "    #and 2 columns(2 bits) \n",
        "    H=F.one_hot(H.long())\n",
        "    #axis=1 to concatenate along columns \n",
        "    basic_data = torch.cat((E, D, H, I),1)\n",
        "\n",
        "    #Derived features\n",
        "    Avg = data[:,4]\n",
        "    Std = data[:,5]\n",
        "    Avg_w = data[:,6]\n",
        "    Std_w = data[:,7]\n",
        "   \n",
        "    Avg[:-1] = Avg[1:].clone()  #by this instead of avarage(or std) btw time T of this day and previous days(depending on window size) it \n",
        "    Std[:-1] = Std[1:].clone()  #becomes avg(or std) of time T of current day and following days\n",
        "    \n",
        "    \n",
        "    Avg=(Avg-torch.min(Avg))/(torch.max(Avg)-torch.min(Avg))\n",
        "    Avg = Avg.reshape(-1, 1)\n",
        "\n",
        "    Std=(Std-torch.min(Std))/(torch.max(Std)-torch.min(Std))\n",
        "    Std = Std.reshape(-1, 1)\n",
        "\n",
        "    Avg_w=(Avg_w-torch.min(Avg_w))/(torch.max(Avg_w)-torch.min(Avg_w))\n",
        "    Avg_w = Avg_w.reshape(-1, 1)\n",
        "\n",
        "    Std_w=(Std_w-torch.min(Std_w))/(torch.max(Std_w)-torch.min(Std_w))\n",
        "    Std_w = Std_w.reshape(-1, 1)\n",
        "\n",
        "\n",
        "    derived_feature = torch.cat((E, Avg_w, Std_w, Avg, Std),1)\n",
        "    \n",
        "    seq_len = window + 1\n",
        "    seq_basic = []\n",
        "    seq_derived = []\n",
        "\n",
        "    for i in range(len(data) - seq_len):\n",
        "      seq_basic.append(basic_data[i: i + seq_len])\n",
        "      seq_derived.append(derived_feature[i: i + window])\n",
        "   \n",
        "    seq_basic=torch.stack(seq_basic) #(n, 3, 58) \n",
        "    seq_derived=torch.stack(seq_derived) #(10782, 2, 5)\n",
        "\n",
        "    x_data = seq_basic[:, :-1] #(n, 2, 58)\n",
        "    y_data = seq_basic[:, -1][:, 0] #(n,1) reading values E\n",
        "\n",
        "    \n",
        "    y_data = (y_data* (E_minmax[1] - E_minmax[0])) + E_minmax[0]  \n",
        "    y_data=y_data.reshape(-1,1)\n",
        "       \n",
        "    return x_data, y_data, seq_derived   #x_data, y_data, seq_derived\n",
        "\n"
      ],
      "metadata": {
        "id": "nGEbQ5STlpgX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "la963BUT_u1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model is defined in pytorch.\n",
        "for each experiment type: 'basic' and 'derived'(proposed method)\n",
        "6 recurrent models are defined as follow:\n",
        "**LSTM  GRU , RNN , BLSTM , BGRU , BRNN**\n"
      ],
      "metadata": {
        "id": "Pts5HlB1PQ33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class recmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, arcitecture_name, model_name):\n",
        "        super(recmodel, self).__init__()\n",
        "        self.arcitecture_name=arcitecture_name\n",
        "        self.input_dim1=58\n",
        "        self.input_dim2=5\n",
        "        self.hidden_dim =20\n",
        "        self.hn1 = Variable(torch.randn(2, 2, 20))\n",
        "        self.hn2 = Variable(torch.randn(2, 2, 20))\n",
        "        \n",
        "        \n",
        "        if arcitecture_name=='basic':\n",
        "          if model_name == \"LSTM\":  \n",
        "            self.lstm = nn.LSTM(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2, batch_first=False)\n",
        "            self.fc1 =  nn.Linear(self.hidden_dim, 20)\n",
        "             \n",
        "          elif model_name == \"RNN\":\n",
        "            self.lstm = nn.RNN(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.fc1 =  nn.Linear(self.hidden_dim, 20)\n",
        "            \n",
        "          elif model_name == \"GRU\":\n",
        "            self.lstm = nn.GRU(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.fc1 =  nn.Linear(self.hidden_dim, 20)\n",
        "            \n",
        "          elif model_name == \"BLSTM\":  \n",
        "            self.lstm = nn.LSTM(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.fc1 =  nn.Linear(self.hidden_dim*2, 20) \n",
        "            \n",
        "          elif model_name == \"BRNN\":\n",
        "            self.lstm = nn.RNN(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.fc1 =  nn.Linear(self.hidden_dim*2, 20)\n",
        "             \n",
        "          elif model_name == \"BGRU\":\n",
        "            self.lstm = nn.GRU(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.fc1 =  nn.Linear(self.hidden_dim*2, 20)  \n",
        "            \n",
        "          self.fc3 =  nn.Linear(20,1)  \n",
        "\n",
        "        elif arcitecture_name=='derived':\n",
        "          \n",
        "          if model_name == \"RNN\":\n",
        "            self.lstm1 = nn.RNN(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.lstm2 = nn.RNN(input_size=self.input_dim2, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.fc2 =  nn.Linear(self.hidden_dim*2, 20) \n",
        "            self.fc3 =  nn.Linear(20,1)\n",
        "          elif model_name == \"LSTM\":  \n",
        "            self.lstm1 = nn.LSTM(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.lstm2 = nn.LSTM(input_size=self.input_dim2, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.fc2 =  nn.Linear(self.hidden_dim*2, 20)\n",
        "            self.fc3 =  nn.Linear(20,1)\n",
        "          elif model_name == \"GRU\":\n",
        "            self.lstm1 = nn.GRU(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.lstm2 = nn.GRU(input_size=self.input_dim2, hidden_size=self.hidden_dim,num_layers=2,batch_first=False)\n",
        "            self.fc2 =  nn.Linear(self.hidden_dim*2, 20)\n",
        "            self.fc3 =  nn.Linear(20,1)\n",
        "          elif model_name == \"BLSTM\":  \n",
        "            self.lstm1 = nn.LSTM(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.lstm2 = nn.LSTM(input_size=self.input_dim2, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.fc2 =  nn.Linear(self.hidden_dim*4, 20)\n",
        "            self.fc3 =  nn.Linear(20,1)\n",
        "          elif model_name == \"BRNN\":\n",
        "            self.lstm1 = nn.RNN(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.lstm2 = nn.RNN(input_size=self.input_dim2, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.fc2 =  nn.Linear(self.hidden_dim*4, 20)\n",
        "            self.fc3 =  nn.Linear(20,1)\n",
        "          elif model_name == \"BGRU\":\n",
        "            self.lstm1 = nn.GRU(input_size=self.input_dim1, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.lstm2 = nn.GRU(input_size=self.input_dim2, hidden_size=self.hidden_dim,num_layers=2,batch_first=False,bidirectional=True)\n",
        "            self.fc2 =  nn.Linear(self.hidden_dim*4, 20)\n",
        "            self.fc3 =  nn.Linear(20,1)\n",
        "\n",
        "        \n",
        "        self.relu= nn.ReLU()\n",
        "        self.drop=nn.Dropout(0.2)\n",
        "              \n",
        "\n",
        "\n",
        "    def forward(self,xb,xd):\n",
        "\n",
        " \n",
        "        if self.arcitecture_name=='basic':\n",
        "          x,self.hn1=self.lstm(xb)\n",
        "          x=self.drop(x[:,-1,:])\n",
        "          x=self.fc1(x)\n",
        "        \n",
        "        elif  self.arcitecture_name=='derived':\n",
        "          xb,self.hn1=self.lstm1(xb) \n",
        "          xb=self.drop(xb)\n",
        "          xd,self.hn2=self.lstm2(xd) \n",
        "          xd=self.drop(xd)\n",
        "          x=torch.cat((xb[:,-1,:],xd[:,-1,:]),1)\n",
        "          x=self.fc2(x)\n",
        "\n",
        "        \n",
        "        x=self.relu(x)\n",
        "        x=self.fc3(x)\n",
        "        return x \n",
        "\n"
      ],
      "metadata": {
        "id": "13nzVw-dAQa6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "VLxvl0nvho20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Class  saves the best model while training. If the current epoch's \n",
        "validation loss is less than the previous least less, then save themodel state."
      ],
      "metadata": {
        "id": "WEq0e4Vy4fwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveBestModel:\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf')\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "        \n",
        "    def __call__(\n",
        "        self, current_valid_loss, \n",
        "        epoch, model, optimizer, criterion ,weight_file\n",
        "    ):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
        "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, weight_file ) #'outputs/best_model.pth'\n",
        "\n"
      ],
      "metadata": {
        "id": "A0o8PPTgO4EP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train** function which receives model train and test datasets as well as directory for saving and loading models parameters and plots"
      ],
      "metadata": {
        "id": "-14bmuL444F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, test_data, weights_file, plots_file):  \n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=False)\n",
        "  test_loader  = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)    \n",
        "    \n",
        "  loss_function = MeanAbsolutePercentageError().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "  \n",
        "  lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose = True)   # in training phase :after validation \n",
        "  best_model=SaveBestModel()    \n",
        "  training_loss = []\n",
        "  test_loss_best=[]\n",
        "  epochs =40\n",
        "  model.to(device)\n",
        " \n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for (xb,xd,y) in tqdm.tqdm(train_loader):\n",
        "          xb,xd,y = xb.to(device),xd.to(device), y.to(device)\n",
        "          optimizer.zero_grad() #zeroing gradient\n",
        "          outputs = model(xb,xd) #forward pass\n",
        "          loss_tr=loss_function(outputs, y) ## obtain the loss function\n",
        "          loss_tr.backward() #calculates the loss of the loss function #retain_graph=True\n",
        "          optimizer.step() #improve from loss, i.e backprop\n",
        "    best_model(loss_tr,epoch,model,optimizer,loss_function,weights_file)        \n",
        "    training_loss.append(loss_tr.item())\n",
        "    if epoch % 5 == 0:\n",
        "           print(f\"Epoch: %d, loss: %1.5f\" % (epoch, loss_tr.item())) \n",
        "           \n",
        "  #save_model(epoch, model, optimizer, loss_function,weights_file)\n",
        "    #print(f'train mape at epoch {epoch}: {loss_tr}')\n",
        "    \"\"\" \n",
        "    model.eval()\n",
        "    #with torch.no_grad():\n",
        "    for (xb,xd,y) in tqdm.tqdm(test_loader):\n",
        "          xb,xd,y = xb.to(device),xd.to(device), y.to(device)\n",
        "          outputs = model(xb,xd) #forward pass\n",
        "          loss_val=loss_function(outputs, y) ## obtain the loss function\n",
        "    validation_loss.append(loss_val.item())\n",
        "    lr_scheduler.step(loss_val)#after validation      \n",
        "    print(f'epoch {epoch} -->  train loss :{loss_tr} , val loss : {loss_val}')     \n",
        "  #save_model(epoch, model, optimizer, loss_function,weights_file)\n",
        "    best_model(loss_val,epoch,model,optimizer,loss_function,weights_file)\n",
        "    \"\"\"\n",
        " \n",
        "  #  Create count of the number of epochs\n",
        "  epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "  # Visualize loss history\n",
        "  plt.clf()\n",
        "  plt.plot(epoch_count, training_loss, 'r--')\n",
        "  #plt.plot(epoch_count, validation_loss, 'b-')\n",
        "  plt.legend(['Training Loss']) #, ['test Loss']\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.savefig(plots_file)\n",
        "\n",
        "  #TO make the evaluation on the est weights saved\n",
        "  best_model_cp = torch.load(weights_file)\n",
        "  model.load_state_dict(best_model_cp['model_state_dict'])     \n",
        "  model.eval()\n",
        " \n",
        " \n",
        "  for (xb,xd,y) in tqdm.tqdm(test_loader):\n",
        "    xb,xd,y = xb.to(device),xd.to(device), y.to(device)\n",
        "    outputs = model(xb,xd) #forward pass\n",
        "    loss=loss_function(outputs, y)\n",
        "    test_loss_best.append(loss.item())\n",
        "\n",
        "  mape=(torch.tensor(test_loss_best,dtype=torch.float32)).mean() \n",
        "  return mape.numpy()"
      ],
      "metadata": {
        "id": "uKHoQ1Ezb86r"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialization\n",
        "\n",
        "experiment_dir = '/content/drive/My Drive/NN project/experiments'\n",
        "data_dir = '/content/drive/My Drive/NN project/data/processed/House1.csv'\n",
        "\n",
        "experiment_type = \"derived\"  #['basic','derived']\n",
        "loss_function = \"mape\"\n",
        "windows = [2] #[2,6,12] \n",
        "model_names = ['LSTM', 'RNN', 'GRU', 'BLSTM', 'BRNN', 'BGRU'] #['LSTM','BLSTM']\n",
        "resume = False\n",
        "dropout = 0.2\n",
        "verbose = 0\n",
        "\n",
        "#For generating graphs\n",
        "labels =  ['Actual', 'LSTM', 'RNN', 'GRU', 'BLSTM', 'BRNN', 'BGRU']  #['Actual', 'LSTM' 'BLSTM']\n",
        "graph_data = {}\n",
        "graph_data[0] = ['*', 'black', 'solid'] #Actual\n",
        "graph_data[1] = ['+', 'green', 'dashed']#LSTM\n",
        "graph_data[2] = ['.', 'yellow', 'dashed']#RNN\n",
        "graph_data[3] = ['^', 'cyan', 'dashdot']#GRU\n",
        "graph_data[4] = ['o', 'orange', 'dashdot']#BLSTM\n",
        "graph_data[5] = ['-', 'red', 'dotted']#BRNN\n",
        "graph_data[6] = ['x', 'blue', 'dotted']#BGRU\n",
        "\n",
        "\n",
        "info = f\"Performed experiment with following parameters: {experiment_type} features, {loss_function} loss function, , dropout {dropout} \\\n",
        "Time_stamps {windows}, models {model_names}, resume {resume} on {datetime.datetime.now()}\"\n"
      ],
      "metadata": {
        "id": "5KxdNF40kGVZ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "lvZXNOdGait6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization** :\n"
      ],
      "metadata": {
        "id": "HZMK_SV3apTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create directories to save plots and results"
      ],
      "metadata": {
        "id": "NLkWEybrbVam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcKLHAOhVFnY",
        "outputId": "983b153b-10eb-48f6-f1ad-0f3846e845bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dir = os.path.join(experiment_dir , 'weights')\n",
        "plots_dir = os.path.join(experiment_dir , 'plots')\n",
        "results_dir = os.path.join(experiment_dir , 'results')\n",
        "graphs_dir = os.path.join(experiment_dir , 'graphs')\n",
        "\n",
        "if(not os.path.isdir(weights_dir)):\n",
        "    os.makedirs(weights_dir)\n",
        "\n",
        "if(not os.path.isdir(plots_dir)):\n",
        "    os.makedirs(plots_dir)\n",
        "\n",
        "if(not os.path.isdir(results_dir)):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "if(not os.path.isdir(graphs_dir)):\n",
        "    os.makedirs(graphs_dir)\n",
        "\n",
        "\n",
        "with open(os.path.join(experiment_dir, \"info.txt\"), \"w\") as f:\n",
        "  f.write(info)\n",
        "  \n"
      ],
      "metadata": {
        "id": "b89sRWaDnA3i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for window in windows:\n",
        "    #generate data and dataset with train,val,test split\n",
        "    train_x, train_y, train_d =get_data(data_ten,'train', window,split) \n",
        "    test_x, test_y, test_d = get_data(data_ten,'test', window,split)\n",
        "\n",
        "    results = pd.DataFrame(columns=['MODEL_NAME', 'MAPE'])   \n",
        "      \n",
        "    for model_name in model_names:\n",
        "\n",
        "        train_data=torch.utils.data.TensorDataset(train_x,train_d,train_y)\n",
        "        test_data=torch.utils.data.TensorDataset(test_x,test_d,test_y)\n",
        "        \n",
        "        if(experiment_type == \"derived\"):\n",
        "            model=recmodel('derived',model_name)\n",
        "\n",
        "        elif(experiment_type == \"basic\"):\n",
        "            model=recmodel('basic',model_name)\n",
        "          \n",
        "        else: \n",
        "            raise ValueError(f'{experiment_type} not defined')\n",
        "\n",
        "        weight_file = os.path.join(weights_dir , str(window) + '_' + model_name + '.pth')  #'.h5'\n",
        "        plot_file = os.path.join(plots_dir , str(window) + '_' + model_name + '.png')\n",
        "       \n",
        "        print(f\"training-->{model_name} with exprimen type:{experiment_type} and window:{window}\")      \n",
        "        mape = train(model, train_data, test_data, weight_file, plot_file) #  doTrain = True, verbose = 1  doTrain = not resume , verbose = verbose\n",
        "        print(f\"MAPE on {model_name} is {mape}\")\n",
        "\n",
        "        results = results.append([{'MODEL_NAME': model_name, 'MAPE': mape}])\n",
        "\n",
        "    results.to_csv(os.path.join(results_dir , str(window) +\".csv\"))\n",
        "   "
      ],
      "metadata": {
        "id": "6154EnI_IF2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_graphs(data, labels, graph_data, dates, graph_file):\n",
        "    formatter = DateFormatter('%Y-%m-%d')\n",
        "    fig, ax = plt.subplots(figsize=(18,8))\n",
        "\n",
        "    for i in range(len(data)):\n",
        "      ax.plot(dates, data[i], graph_data[i][0], color=graph_data[i][1], linestyle=graph_data[i][2])\n",
        "\n",
        "    plt.ylabel ('KW per 30 minutes', size = 20)\n",
        "    plt.xticks(size = 20)\n",
        "    plt.yticks(size = 20)\n",
        "\n",
        "    plt.gcf().axes[0].xaxis.set_major_formatter(formatter)\n",
        "\n",
        "    for label in plt.gcf().axes[0].xaxis.get_ticklabels()[::2]:\n",
        "      label.set_visible(False)\n",
        "    \n",
        "    ax.legend(labels, loc='upper center', bbox_to_anchor=(0.5, 1.13),\n",
        "            ncol=3, fancybox=True, shadow=True, fontsize=18)\n",
        "    \n",
        "    plt.savefig(graph_file, format='pdf', bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9JSa6W2MVy6q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates = mdates.drange(datetime.datetime(2019, 4, 28, 0, 0), datetime.datetime(2019, 5, 1, 0, 0),  datetime.timedelta(minutes=30))\n",
        "\n",
        "for window in windows:\n",
        "  \n",
        "    test_x = test_x[144:288]  \n",
        "    test_d = test_d[144:288] \n",
        "    test_y = test_y[144:288] \n",
        "\n",
        "    data = []\n",
        "    data.append(test_y)\n",
        "\n",
        "    for model_name in model_names:\n",
        "    \n",
        "        if(experiment_type == \"derived\"):\n",
        "              model=recmodel('derived',model_name)\n",
        "              \n",
        "\n",
        "        elif(experiment_type == \"basic\"):\n",
        "              model=recmodel('basic',model_name)\n",
        "\n",
        "        weight_file = os.path.join(weights_dir , str(window) + '_' + model_name + '.pth') #.h5\n",
        "        model.eval()\n",
        "        model_cp = torch.load(weight_file)\n",
        "        model.load_state_dict(model_cp['model_state_dict'])\n",
        "        \n",
        "        #model.load_weights(weight_file)\n",
        "        with torch.no_grad():\n",
        "          data.append(model(test_x,test_d))\n",
        "    create_graphs(data, labels, graph_data, dates, os.path.join(graphs_dir, str(window)+\".pdf\"))"
      ],
      "metadata": {
        "id": "Iff4vLoMu2J5"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5nhmezDKLxs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "FQIalohjCEKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plots folder in experiment directory contains (training loss-epochs) plots.** \n",
        "Due to the volatile nature of data training is done only on training set without spiliting it to train+validation sets.\n",
        "After trying different ways of spiliting the training data at the end training without validation set resulted better MAPE/.\n",
        "**for example for RNN with window=2:**\n",
        " \n",
        "\n",
        " <figure>\n",
        " <center>\n",
        " <img src='https://i.postimg.cc/wMjn9Xt5/2-RNN.png' width=\"800\" \n",
        "      height=\"500\"/>\n",
        " <figcaption>Training loss</figcaption></center>\n",
        " </figure>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egbNSJR99721"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "graph folder in experiment directory contains actual and predicted value for a portion of testset from 2019-04-28\n",
        "to 2019-05-01\n",
        "\n",
        " <figure>\n",
        " <center>\n",
        " <img src='https://i.postimg.cc/85PyqLnh/2.png' width=\"1200\" \n",
        "      height=\"500\"/>\n",
        " <figcaption>Training loss</figcaption></center>\n",
        " </figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xW4mlZOYW3M6"
      }
    }
  ]
}